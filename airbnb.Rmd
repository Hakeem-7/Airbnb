---
title: "Airbnb"
author: "Akeem Ajede"
date: "4/22/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Visualization

```{r}
listing <- as_tibble(read.csv("files/listings.csv"))
head(listing)

dim(listing)

min(listing$price) #zero listing price should be eliminated - possible missing data

# convert 0's to NA
listing[, "price"][listing[, "price"] == 0] <- NA

is.na(listing) %>% sum() # 21 percent of the price entry is "zero"
dim(listing)

# Rental price range of each neighbourhood

price_var <- listing %>%
  group_by(neighbourhood) %>%
  summarise_each(funs(min(.,na.rm = T), max(., na.rm = T)),
                 matches("pric")) %>% rename("price_min" = min, "price_max" = max) %>%
  arrange(desc(price_max))%>%
  DT::datatable()

# average rental price in each neighbourhood
options(digits = 4)
price_avg <- listing %>%
  group_by(neighbourhood) %>%
  summarise_each(funs(mean),matches("pric")) %>% 
  rename("Average_price" = price) %>%
  arrange(desc(Average_price)) %>%
  mutate_if(is.numeric, format, 2) %>%
  DT::datatable()

price_avg

```

# Pre-processing and Feature Selection

```{r}
#pre-processing

# Replaced rentals with no cleaning fee with $0.
# Rentals with no review score rating were assigned a score of 99


det_listing <- read.csv("files/Detailed_listings.csv", header = TRUE)

glimpse(det_listing)

# Features selection using knowledge gained from literature reviews

keeps <- c("host_id", "host_name", "neighbourhood_cleansed", "accommodates", "bathrooms", "bedrooms", "beds", "price","cleaning_fee",
           "minimum_nights","maximum_nights", "review_scores_rating")

listing_v <- det_listing[keeps] %>%
  rename("zip_code" = neighbourhood_cleansed)%>%
  select(-price, price) # Push price variable to the back

#glimpse(listing_v)
dim(listing_v)

# function for determining variable class
class_variable <- function(data){
  sapply(data, function(x) class(x))
}

listing_v$zip_code <- as.factor(listing_v$zip_code)
class_variable(listing_v)

# 0.8% of the data is missing data
sum(is.na(listing_v))

# remove missing data
listing_v <- na.omit(listing_v)

# Data Visualization
library(ggcorrplot)
library(corrplot)
ggcorrplot(cor(listing_v[,-c(1,2,3)]))
corrplot(cor(listing_v[,-c(1,2,3)]))
```

## Feature selection using lasso regression

```{r}
# Feature selection using lasso regression

keeps1 <- c("accommodates", "bathrooms", "bedrooms", "beds", "price","cleaning_fee",
           "minimum_nights","maximum_nights", "review_scores_rating")

listing_v1 <- det_listing[keeps1] %>%
  select(price, everything()) #Move the variable "price" to the first column

sum(is.na(listing_v1))

listing_v1 <- na.omit(listing_v1)

head(listing_v1)
dim(listing_v1)

# Lasso Regression

library(glmnet)

x <- model.matrix(price~., listing_v1)[,-1]
y <- listing_v1$price

# Sample splitting
set.seed(7)

train.r<-sample(1:nrow(x), 0.7*nrow(x))
test.r<-(-train.r)
y.test<-y[test.r]

# Generate the model

grid<- 10^seq(6,-2,length = 100)

lasso <- glmnet(x[train.r,], y[train.r], alpha = 1, lambda = grid, standardize = TRUE) #alpha = 1 is lasso
plot(lasso)

# Cross-validation for selecting the best lambda
set.seed(7)
cv.out<-cv.glmnet(x[train.r, ],y[train.r],alpha = 1)
plot(cv.out)
bestlam<-cv.out$lambda.min
bestlam
lasso.pred<-predict(lasso, s=0, newx = x[test.r,])
#test MSE is 2,164,444. when s = 0 (i.e., least regression), the MSE is 2,165,929, which is
#significantly higher than the MSE of the Lasso regression
mean((lasso.pred - y.test)^2) 

lasso.out<-glmnet(x,y,alpha = 1,lambda = grid)
lasso.coef<-predict(lasso.out, type = "coefficients", s=bestlam)[2:9,]
lasso.coef
# None of the features was eliminated. Thus, all features are relevant for modelling.
lasso.coef[lasso.coef!=0]
```


## Support Vector Regression

```{r}

# sample splitting
train_id <- sample(nrow(listing_v1), 0.7*nrow(listing_v1))
train <- listing_v1[train_id, ]
test <- listing_v1[-train_id,]

library(e1071)
set.seed(7)
svm_model <- svm(train$price~., data = train, kernel = "radial", scale = TRUE, type = "eps-regression",
                 sigma = 0.6305, cost = 1) #radial basis function

summary(svm_model)

library(caret)
library(lattice)
library(kernlab)

# selecting the optimum sigma and cost value

control <- trainControl(method = "repeatedcv", number = 5, repeats = 2)
set.seed(1016)
model <- train(price~., data = train, method = "svmRadialCost", trControl = control)
plot(model)

model <- train(price~., data = train, method = "svmRadialSigma", trControl = control)
plot(model)
print(model)

model <- train(price~., data = train, method = "svmRadial", trControl = control)
plot(model)

#Confusion Matrix
pred <- predict(svm_model, test)
pred
table <- data.frame(test$price, pred)
table
rmse <- sqrt((sum(pred - test$price)^2))
# the root mean square error of the model is 721,507. That is significantly lower than Lasso.
rmse
```


## Neural Network with backpropagation

```{r}
library(neuralnet)

# Scaling
# Create a normalization function.
normalize <- function(x){
  y = ((x - min(x))/(max(x)-min(x)))
  return(y)
}

listing_norm <- as.data.frame(lapply(listing_v1, normalize)) #Normalizing a dataframe

# sample splitting
train_id <- sample(nrow(listing_norm), 0.7*nrow(listing_norm))
train_norm <- listing_norm[train_id, ]
test_norm <- listing_norm[-train_id,]

# Activation Function

softplus <- function(x)log(1+exp(x))
relu <- function(x) sapply(x, function(z) max(0,z))

x <- seq(from=-10, to=10, by=0.2)
library(ggplot2)
library(reshape2)

fits <- data.frame(x=x, softplus = softplus(x), relu = relu(x))
#glimpse(fits)
long <- melt(fits, id.vars="x") 
#glimpse(long)
ggplot(data=long, aes(x=x, y=value, group=variable, colour=variable))+
  geom_line(size=1) +
  ggtitle("ReLU & Softplus") +
  theme(plot.title = element_text(size = 26, hjust = 0.5)) +
  theme(legend.title = element_blank()) +
  theme(legend.text = element_text(size = 18)) #Almost same as ReLU

# Model
set.seed(7)
nn_model <- neuralnet(train_norm$price~., data = train_norm, hidden = 5, linear.output = FALSE,
                      act.fct = softplus)
plot(nn_model)

# Model evaluation
nn_pred <- compute(nn_model, test_norm[-1])
pred1 <- nn_pred$net.result

# correlation is used instead of confusion matrix - Not a classification problem

#accuracy is 87.2% using sigmoid activation function.
cor(pred1,test_norm$price)

```




